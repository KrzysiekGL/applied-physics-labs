{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Języki Programowania Python i R\n",
    "\n",
    "\n",
    "## dr inż. Patryk Jasik\n",
    "### Division of Theoretical Physics and Quantum Information\n",
    "### Institute of Physics and Computer Science\n",
    "### Faculty of Applied Physics and Mathematics\n",
    "### Gdansk University of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DALEX\n",
    "## https://dalex.drwhy.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "## !pip install dalex -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the necessary packages\n",
    "import dalex as dx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version of Dalex\n",
    "dx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataset - measurements of physical and chemical properties of Portuguese Vinho Verde wines (white and red)\n",
    "wine = pd.read_csv(\"data/winequality-all.csv\", comment=\"#\")\n",
    "wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goal - creation of regression model and explanation of its local and global features\n",
    "### target variable: alcohol\n",
    "### predictors: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors\n",
    "#we will leave out three observations (first, before last, and last) to test explanation methods on them\n",
    "\n",
    "X = wine.iloc[1:-2, 0:12].drop(columns=\"alcohol\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the target variable\n",
    "y = wine.iloc[1:-2, -3]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will create the pipeline in order to perform automatic preprocessing, which will allow us to prepare the dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create the transformer for numerical features.\n",
    "#We can use any transformation of predictors here and in this case, it will be standardization.\n",
    "\n",
    "numerical_features = ['fixed.acidity',\n",
    "                      'volatile.acidity',\n",
    "                      'citric.acid',\n",
    "                      'residual.sugar',\n",
    "                      'chlorides',\n",
    "                      'free.sulfur.dioxide',\n",
    "                      'total.sulfur.dioxide',\n",
    "                      'density',\n",
    "                      'pH',\n",
    "                      'sulphates',\n",
    "                      'response']\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()) #the name of the step in pipeline \n",
    "    ]\n",
    ")\n",
    "\n",
    "#numerical_transformer = Pipeline(\n",
    "#    steps=[\n",
    "#        ('scaler', MinMaxScaler())\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of transformer for numerical features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create the regression model using Multi-layer Perceptron (MLP) \n",
    "#regMLP = MLPRegressor(hidden_layer_sizes=(150,100,50), max_iter=500, random_state=0)\n",
    "regkNN = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final pipeline consists of two steps: preprocessor and regressor\n",
    "\n",
    "reg = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', regkNN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An example of more complicated pipeline\n",
    "\n",
    "numerical_features = ['energy', 'wavelength', 'intensity', 'reduced_mass']\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_features = ['multiplicity', 'calc_method', 'bond_type']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=500, random_state=0)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's train the MLP model using the created pipeline.\n",
    "\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is time to create the Dalex explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black-box models may have very different structures. This class creates a unified representation of a model, which can be further processed by various explanations. Methods of this class produce explanation objects, that contain the main result attribute, and can be visualised using the plot method.\n",
    "\n",
    "![title](local_global_explanations.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = dx.Explainer(reg, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will check the local explanations using previously skipped observations.\n",
    "\n",
    "first = wine[wine.index == wine.index.min()].iloc[:, 0:12].drop(columns='alcohol')\n",
    "before_last = wine[wine.index == wine.index.max()-1].iloc[:, 0:12].drop(columns='alcohol')\n",
    "last = wine[wine.index == wine.index.max()].iloc[:, 0:12].drop(columns='alcohol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's compare the original values of the target variable with the predicted ones.\n",
    "y[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.predict(X)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First original\", wine.alcohol[0])\n",
    "print(\"Before last original\", wine.alcohol[5318])\n",
    "print(\"Last original\", wine.alcohol[5319])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First prediction\", exp.predict(first).round(1))\n",
    "print(\"Before last prediction\", exp.predict(before_last).round(1))\n",
    "print(\"Last prediction\", exp.predict(last).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction level - Local explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break Down method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to calculate the contribution of variable in prediction of f(x) as changes in the expected model response given other variables. This means that we start with the mean expected model response of the model, successively adding variables to the conditioning. Of course, the order in which the variables are arranged also influences the contribution values. If our model is additive, the arrangement of individual variables and values will be the same. If we have a non-additive model with p variables, we have p! orders, it is complicated by calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The predict_parts function calculates predict-level variable attributions as Break Down, Shapley Values, or Shap Values\n",
    "#Let's look on the Break Down results\n",
    "\n",
    "bd_first = exp.predict_parts(first, type='break_down', label=\"first\")\n",
    "bd_interactions_first = exp.predict_parts(first, type='break_down_interactions', label=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_first.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_before_last = exp.predict_parts(before_last, type='break_down', label=\"before_last\")\n",
    "bd_interactions_before_last = exp.predict_parts(before_last, type='break_down_interactions', label=\"before_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_before_last.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_last = exp.predict_parts(last, type='break_down', label=\"last\")\n",
    "bd_interactions_last = exp.predict_parts(last, type='break_down_interactions', label=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_last.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_first.plot(max_vars=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_before_last.plot(max_vars=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_last.plot(max_vars=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapley value is a model agnostic method, we can use it for any type of model. The benefit of Shapley values is additive feature attribution property. It is a local explanation. In the comparison with the Break Down, the Shapley value is a generalization because in Break Down method represents one of all variable orders. Now, we consider all orders for variables, so if we have the p features in our dataset, then we have p! orders. The output is averaging the possible orders.\n",
    "\n",
    "The Shapley value method is based on Break Down predictions into parts. This is a slightly different approach than in the Break Down method. It is based on the idea of averaging the input value of a given variable overall or a large number of possible orders.\n",
    "\n",
    "An important practical limitation of the general model-agnostic method is that, for large models, the calculation of Shapley values is time-consuming. In specific situations, they can be calculated very quickly. For example, for additonal models and for models based on trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the case of 10 predictors we have a lot of possibilities :)\n",
    "import math\n",
    "math.factorial(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look on the Shapley values\n",
    "#B is number of random paths to calculate variable attributions (default is 25)\n",
    "# B=100 :)\n",
    "sh_first = exp.predict_parts(first, type='shap', label='first', B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_first.result.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_first.result.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_first.result.loc[sh_first.result.B == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_first.plot(bar_width = 16, max_vars=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_before_last = exp.predict_parts(before_last, type='shap', label='before_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_before_last.result.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_before_last.result.loc[sh_before_last.result.B == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_before_last.plot(bar_width = 16, max_vars=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_last = exp.predict_parts(last, type='shap', label='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_last.result.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_last.result.loc[sh_last.result.B == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_last.plot(bar_width = 16, max_vars=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceteris Paribus profiles\n",
    "### \"all other things being equal\" or \"other things held constant\" or \"all else unchanged\"\n",
    "### https://en.wikipedia.org/wiki/Ceteris_paribus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceteris Paribus is a Latin phrase meaning “other things held constant” or “all else unchanged”. Ceteris Paribus (CP) profiles are designed to show model response around a single point in the feature space. They show how the model response depends on changes in a single input variable, keeping all other variables unchanged. They work for any Machine Learning model and allow for model comparisons to better understand how a model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create the Ceteris Paribus profiles\n",
    "cp_first = exp.predict_profile(first, label=\"first\")\n",
    "cp_before_last = exp.predict_profile(before_last, label=\"before_last\")\n",
    "cp_last = exp.predict_profile(last, label=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_first.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_before_last.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_last.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cp_first.plot([cp_before_last, cp_last])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model level - Global explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate the model-level model performance measures\n",
    "mp = exp.model_performance(model_type = 'regression')\n",
    "mp.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the extraction of R2 metric\n",
    "mp.result.r2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation-based variable importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is very simple, to assess how important is the variable V we will compare the initial model with the model on which effect of the variable V is removed. How to remove the effect of variable V? In permutation-based variable-importance method, the effect of a variable is removed though a random reshuffling of the data. We take the original data, then we permutate, and we get “new” data, on which we calculate the prediction.\n",
    "\n",
    "**If a variable is important in a model, then after its permutation the model prediction should be less precise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate model-level variable importance.\n",
    "vi = exp.model_parts(loss_function='rmse', N=None, B=100)\n",
    "vi.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.plot(max_vars=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_grouped = exp.model_parts(variable_groups={'sulfur': ['free.sulfur.dioxide', 'total.sulfur.dioxide', 'sulphates'],\n",
    "                                              'acid': ['fixed.acidity', 'volatile.acidity', 'citric.acid'],\n",
    "                                              'sugar': ['residual.sugar'],\n",
    "                                              'density': ['density'],\n",
    "                                              'other': ['chlorides', 'pH', 'response']}, loss_function='rmse',\n",
    "                             N=None, B=100)\n",
    "vi_grouped.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_grouped.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind the design of PD profiles is to show how the expected model prediction value behaves as a function of the selected explanatory variable. For one model we can construct a general PD profile using all observations from the data set or several profiles for observation subgroups. A comparison of subgroup-specific profiles can provide important insight, for example, into the stability of the model prediction.\n",
    "\n",
    "1. Profiles can be created for all the observations in the set, as well as for the division against other variables. For example, we can see how a specific variable behaves when differentiated by energy, intensity, or other factors.\n",
    "2. We can detect some complicated variable relationships. For example, we have PD profiles for two models and we can see that one of the simple models (linear regression) does not detect any dependence, while the profile for a black-box model (random forest) notices a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions calculate explanations that explore model response as a function of selected variables.\n",
    "\n",
    "#partial-dependence profile\n",
    "pdp_num = exp.model_profile(type = 'partial', label=\"pdp\", N=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdp_num.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP profile for residual.sugar and pH variable\n",
    "pdp_num = exp.model_profile(variables = [\"residual.sugar\", \"pH\"] , type = \"partial\", label=\"pdp\", N=None)\n",
    "\n",
    "# plot PDP\n",
    "pdp_num.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP profile for residual.sugar and pH variable grouped by response\n",
    "pdp_num_group = exp.model_profile(variables = [\"residual.sugar\", \"pH\"], groups = \"response\", type = \"partial\", N=None)\n",
    "\n",
    "# plot PDP\n",
    "pdp_num_group.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes, You can :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
