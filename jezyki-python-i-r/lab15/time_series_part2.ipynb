{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35de7da9",
   "metadata": {},
   "source": [
    "# Języki Programowania Python i R\n",
    "\n",
    "\n",
    "## dr inż. Patryk Jasik\n",
    "### Division of Theoretical Physics and Quantum Information\n",
    "### Institute of Physics and Computer Science\n",
    "### Faculty of Applied Physics and Mathematics\n",
    "### Gdansk University of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ładowanie bibliotek\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as metr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ustawienia wykresów i wyświetlanie liczb zmiennoprzecinkowych\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "np.set_printoptions(precision=8, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbeb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelowanie szeregów czasowych\n",
    "#klasyczny podział na zbiór treningowy i testowy oraz walidacja krzyżowa nie będę tutaj skuteczne\n",
    "\n",
    "\n",
    "#Podział szeregu na zbiór treningowy i testowy - backtesting lub hindcasting\n",
    "#train-test split - z zachowaniem chronologii szeregu\n",
    "\n",
    "series = read_csv('dane/sunspots.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "X = series.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[0:train_size], X[train_size:len(X)]\n",
    "print('Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(train)))\n",
    "print('Testing Observations: %d' % (len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i jeszcze wykres zbioru treningowego i testowego\n",
    "plt.plot(train)\n",
    "plt.plot([None for i in train] + [x for x in test])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82457bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi train-test split - podział szeregu czasowego na kilka zbiorów\n",
    "#coś w rodzaju walidacji krzyżowej - metoda KFold\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "series = read_csv('dane/sunspots.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "X = series.values\n",
    "splits = TimeSeriesSplit(n_splits=4)\n",
    "plt.figure(1)\n",
    "index = 1\n",
    "for train_index, test_index in splits.split(X):\n",
    "    train = X[train_index]\n",
    "    test = X[test_index]\n",
    "    print('Observations: %d' % (len(train) + len(test)))\n",
    "    print('Training Observations: %d' % (len(train)))\n",
    "    print('Testing Observations: %d' % (len(test)))\n",
    "    plt.subplot(410 + index)\n",
    "    plt.plot(train)\n",
    "    plt.plot([None for i in train] + [x for x in test])\n",
    "    index += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#konstrukcja metody postępowania w przewidywaniu wartości szeregu czasowego\n",
    "#budowa modelu referencyjnego i określenie wartości referencyjnej (bazowej) błędu\n",
    "\n",
    "#1. wczytanie zbioru\n",
    "def parser(x):\n",
    "    return datetime.strptime('202'+x, '%Y-%m')\n",
    "\n",
    "series = read_csv('dane/shampoo-sales.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True, date_parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83385548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. tworzymy zmienną, która jest naszym szeregiem przesuniętym o jeden krok czasowy\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. dzielimy szereg na część uczącą i testową\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "\n",
    "test_X, test_y = test[:,0], test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde72179",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1766b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. tworzymy funkcję zwracającą wartość predykcji modelu naiwnego\n",
    "def model_persistence(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. testujemy model za pomocą metody walidacji postępującej\n",
    "predictions = list()\n",
    "for x in test_X:\n",
    "    yhat = model_persistence(x)\n",
    "    predictions.append(yhat)\n",
    "\n",
    "rmse = np.sqrt(metr.mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1470d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. wyświtalmy wyniki predykcji i dane oryginalne\n",
    "plt.plot(train_y)\n",
    "plt.plot([None for i in train_y] + [x for x in test_y])\n",
    "plt.plot([None for i in train_y] + [x for x in predictions])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocena jakości modeli predykcyjnych poprzez wizualizację\n",
    "\n",
    "#budujemy naiwny model predykcyjny\n",
    "series = read_csv('dane/daily-total-female-births.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True)\n",
    "\n",
    "# tworzymy nową zmienną \n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# dzielimy szereg na zbiór treningowy i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predykcja\n",
    "predictions = [x for x in test_X]\n",
    "\n",
    "# wyznaczamy błędy (residuals)\n",
    "residuals = [test_y[i]-predictions[i] for i in range(len(predictions))]\n",
    "residuals = pd.DataFrame(residuals)\n",
    "print(residuals.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liniowy wykres błędów\n",
    "residuals.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statystyki błędów predykcji\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram błędów i ich rozkład\n",
    "residuals.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.plot(kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wykres Q-Q albo wykres kwantylowy\n",
    "#możemy sprawdzić w ten sposób nomralność rozkładu błędów\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "series = read_csv('dane/daily-total-female-births.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True)\n",
    "\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "predictions = [x for x in test_X]\n",
    "\n",
    "residuals = [test_y[i]-predictions[i] for i in range(len(predictions))]\n",
    "residuals = np.array(residuals)\n",
    "\n",
    "qqplot(residuals, line='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wykres autokorelacji\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(residuals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f857449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inżynieria cech dla szeregów czasowych\n",
    "\n",
    "#zagadnienie regresyjne\n",
    "#wczytujemy dane\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "#tworzymy nową cechę poprzez przesunięcie danych w czasie\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "#budujemy cechę jako najbliższą mniejszą liczbę podzielną przez 5 \n",
    "for i in range(len(dataframe['t+1'])):\n",
    "    dataframe['t+1'][i] = int(dataframe['t+1'][i] / 5) * 5.0\n",
    "\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2071f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#budowanie cech dla klasyfikacji\n",
    "\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                  index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "#dyskretyzujemy zmienną ciągłą\n",
    "for i in range(len(dataframe['t+1'])):\n",
    "    value = dataframe['t+1'][i]\n",
    "    if value < 10.0:\n",
    "        dataframe['t+1'][i] = 0\n",
    "    elif value >= 20.0:\n",
    "        dataframe['t+1'][i] = 2\n",
    "    else:\n",
    "        dataframe['t+1'][i] = 1\n",
    "\n",
    "dataframe['t+1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f46a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#budowanie cech dla predykcji dłuższych horyzontów czasowych\n",
    "\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "# tworzymy zbiór przesuniętych w czasie cech\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values, values.shift(-1),\n",
    "                       values.shift(-2), values.shift(-3),\n",
    "                       values.shift(-4), values.shift(-5),\n",
    "                       values.shift(-6)], axis=1)\n",
    "dataframe.columns = ['t', 't+1', 't+2', 't+3', 't+4', 't+5', 't+6', 't+7']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ac670",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe674d5",
   "metadata": {},
   "source": [
    "### https://tsfresh.readthedocs.io/en/latest/index.html\n",
    "### https://tsfel.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "aaa = datetime.date(2022, 4, 3).isocalendar()\n",
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELOWANIE\n",
    "\n",
    "#Model autoregresyjny -> X(t+1) = b0 + b1*X(t) + b2*X(t-1)\n",
    "\n",
    "\n",
    "#przed przystąpieniem do modelowania sprawdzamy czy w szeregu występują korelacje\n",
    "\n",
    "from pandas.plotting import lag_plot\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "lag_plot(series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a43d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sprawdzamy wielkość korelacji i tworzymy wykres autokorelacji (dwie wersje)\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "result = dataframe.corr()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(series)\n",
    "plt.xlim(0,30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(series, lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d93482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#budujemy naiwny model bazowy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "#tworzymy zmienną przesuniętą w czasie\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "#tworzymy zbiór treningowy i testowy\n",
    "X = dataframe.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model bazowy\n",
    "def model_persistence(x):\n",
    "    return x\n",
    "\n",
    "#walidacja krocząca\n",
    "predictions = list()\n",
    "for x in test_X:\n",
    "    yhat = model_persistence(x)\n",
    "    predictions.append(yhat)\n",
    "\n",
    "#obliczamy błąd predykcji\n",
    "rmse = sqrt(mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "#wizualiazacja predykcji\n",
    "plt.plot(test_y)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977df247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model AR\n",
    "\n",
    "#wczytujemy odpowiedni pakiet\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "#tworzymy zbiór treningowy i testowy\n",
    "X = series.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "\n",
    "#budujemy model autoregresyjny\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)\n",
    "print(len(model_fit.params))\n",
    "\n",
    "#predykcja\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# wykres predykcji\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cd5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#budowanie modelu AR dla nowych wartości\n",
    "#korzystamy ze współczynników bazowego modelu AR\n",
    "#yhat = b0 + b1*X1 + b2*X2 + ... + bn*Xn\n",
    "\n",
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "#tworzymy zbiór treningowy i testowy\n",
    "X = series.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "\n",
    "#budujemy model autoregresyjny\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "window = model_fit.k_ar\n",
    "coef = model_fit.params\n",
    "\n",
    "\n",
    "#robimy predykcję kolejnych punktów ze zbioru testowego,\n",
    "#korzystając ze współczynników wyznaczonych dla zbioru treningowego\n",
    "history = train[len(train)-window:]\n",
    "history = [history[i] for i in range(len(history))]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    length = len(history)\n",
    "    lag = [history[i] for i in range(length-window,length)]\n",
    "    yhat = coef[0]\n",
    "    for d in range(window):\n",
    "        yhat += coef[d+1] * lag[window-d-1]\n",
    "    obs = test[t]\n",
    "    predictions.append(yhat)\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "#wykres predykcji\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tworzymy model naiwny\n",
    "\n",
    "# wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-total-female-births.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True)\n",
    "\n",
    "# tworzenie cechy dla modelu naiwnego\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model naiwny\n",
    "predictions = [x for x in test_X]\n",
    "\n",
    "# sprawdzamy jakość modelu naiwnego\n",
    "rmse = sqrt(mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# wyznaczamy błędy (reszty)\n",
    "residuals = [test_y[i]-predictions[i] for i in range(len(predictions))]\n",
    "residuals = pd.DataFrame(residuals)\n",
    "print(residuals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mamy teraz szereg reszt, który możemy modelować za pomocą modeli AR\n",
    "\n",
    "#error(t+1) = b0 + b1*error(t) + b2*error(t-1) + ... + bn*error(t-n)\n",
    "\n",
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-total-female-births.csv', header=0,\n",
    "                     index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "# tworzenie cechy dla modelu naiwnego\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model naiwny\n",
    "train_pred = [x for x in train_X]\n",
    "\n",
    "# obliczamy reszty\n",
    "train_resid = [train_y[i]-train_pred[i] for i in range(len(train_pred))]\n",
    "\n",
    "# trenujemy model AR dla reszt\n",
    "model = AR(train_resid)\n",
    "model_fit = model.fit()\n",
    "window = model_fit.k_ar\n",
    "coef = model_fit.params\n",
    "print('Lag=%d, Coef=%s' % (window, coef))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976ad9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wykorzystamy wyniki do predykcji błędu (reszt)\n",
    "\n",
    "# wykorzystujemy walidację kroczącą dla zbioru testowego z wykorzystaniem okna\n",
    "history = train_resid[len(train_resid)-window:]\n",
    "history = [history[i] for i in range(len(history))]\n",
    "predictions = list()\n",
    "expected_error = list()\n",
    "for t in range(len(test_y)):\n",
    "    # model naiwyny\n",
    "    yhat = test_X[t]\n",
    "    error = test_y[t] - yhat\n",
    "    expected_error.append(error)\n",
    "    # predykcja błędu\n",
    "    length = len(history)\n",
    "    lag = [history[i] for i in range(length-window,length)]\n",
    "    pred_error = coef[0]\n",
    "    for d in range(window):\n",
    "        pred_error += coef[d+1] * lag[window-d-1]\n",
    "    predictions.append(pred_error)\n",
    "    history.append(error)\n",
    "    print('predicted error=%f, expected error=%f' % (pred_error, error))\n",
    "\n",
    "# wykres reszt\n",
    "plt.plot(expected_error)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f7eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#poprawiamy naiwną predykcję przy użyciu predykcji reszt\n",
    "#improved_forecast = forecast + estimated_error\n",
    "\n",
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-total-female-births.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True)\n",
    "\n",
    "# tworzenie cechy dla modelu naiwnego\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model naiwny\n",
    "train_pred = [x for x in train_X]\n",
    "\n",
    "# obliczamy reszty\n",
    "train_resid = [train_y[i]-train_pred[i] for i in range(len(train_pred))]\n",
    "\n",
    "# trenujemy model AR dla reszt\n",
    "model = AR(train_resid)\n",
    "model_fit = model.fit()\n",
    "window = model_fit.k_ar\n",
    "coef = model_fit.params\n",
    "\n",
    "# wykorzystujemy walidację kroczącą dla zbioru testowego z wykorzystaniem okna\n",
    "history = train_resid[len(train_resid)-window:]\n",
    "history = [history[i] for i in range(len(history))]\n",
    "predictions = list()\n",
    "for t in range(len(test_y)):\n",
    "    # model naiwyny\n",
    "    yhat = test_X[t]\n",
    "    error = test_y[t] - yhat\n",
    "    # predykcja reszt\n",
    "    length = len(history)\n",
    "    lag = [history[i] for i in range(length-window,length)]\n",
    "    pred_error = coef[0]\n",
    "    for d in range(window):\n",
    "        pred_error += coef[d+1] * lag[window-d-1]\n",
    "    # poprawiamy predykcję przy użyciu reszt\n",
    "    yhat = yhat + pred_error\n",
    "    predictions.append(yhat)\n",
    "    history.append(error)\n",
    "    print('predicted=%f, expected=%f' % (yhat, test_y[t]))\n",
    "\n",
    "# błąd predykcji\n",
    "rmse = sqrt(mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# wykres predykcji\n",
    "plt.plot(test_y)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64625aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA - AutoRegressive Integrated Moving Average\n",
    "#AR I(różnicowanie) MA\n",
    "#ARIMA(p,d,q)\n",
    "#p - rzęd autoregresji (liczba wcześniejszych wartości)\n",
    "#d - rząd różnicowania\n",
    "#q - rząd średniej ruchomej\n",
    "\n",
    "\n",
    "# wczytujemy dane\n",
    "def parser(x):\n",
    "    return pd.datetime.strptime('201'+x, '%Y-%m')\n",
    "\n",
    "series = pd.read_csv('dane/shampoo-sales.csv', header=0, index_col=0,\n",
    "                     parse_dates=True, squeeze=True, date_parser=parser)\n",
    "\n",
    "print(series.head())\n",
    "\n",
    "# wykresy\n",
    "series.plot()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd231c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#na podstawie wykresu autokorelacji możemy ustalić lag dla modelu ARIMA (jaki?)\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# budujemy model i trenujemy\n",
    "model = ARIMA(series, order=(6, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# podsumowanie trenowania\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d99829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wykresy reszt\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51769ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rozkład reszt\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# statystyki reszt\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25d77a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# zbiór uczący i testowy\n",
    "X = series.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walidacja krocząca\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(6, 1, 1))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "# błąd predykcji\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# wykres predykcji\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzenie cechy dla modelu naiwnego\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model naiwny\n",
    "predictions = [x for x in test_X]\n",
    "\n",
    "# sprawdzamy jakość modelu naiwnego\n",
    "rmse = sqrt(mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4763a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-total-female-births.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True)\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = series.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walidacja krocząca\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "# błąd predykcji\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# wykres predykcji\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzenie cechy dla modelu naiwnego\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model naiwny\n",
    "predictions = [x for x in test_X]\n",
    "\n",
    "# sprawdzamy jakość modelu naiwnego\n",
    "rmse = sqrt(mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aca14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wczytywanie danych\n",
    "series = pd.read_csv('dane/daily-minimum-temperatures.csv', header=0, index_col=0,\n",
    "                  parse_dates=True, squeeze=True)\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = series.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walidacja krocząca\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "# błąd predykcji\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# wykres predykcji\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dca054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzenie cechy dla modelu naiwnego\n",
    "values = pd.DataFrame(series.values)\n",
    "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# zbiór uczący i testowy\n",
    "X = dataframe.values\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[1:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# model naiwny\n",
    "predictions = [x for x in test_X]\n",
    "\n",
    "# sprawdzamy jakość modelu naiwnego\n",
    "rmse = sqrt(mean_squared_error(test_y, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96572bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ładowanie bibliotek\n",
    "from pandas import read_csv\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as metr\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.ar_model import ARResults #do wczytywania modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytujemy dane\n",
    "series = read_csv('dane/daily-minimum-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "#długość szeregu czasowego\n",
    "print(series.shape)\n",
    "\n",
    "#funkcja ACF z zaznaczonym 95% przedziałem ufności\n",
    "#wartości na zewnątrz tego obszaru są najprawdopodobniej prawdziwymi korelacjami,\n",
    "#natomiast wewnątrz mogą być statystycznym szumem\n",
    "\n",
    "#Wiemy, że ACF opisuje autokorelację między obserwacją a inną obserwacją we wcześniejszym kroku czasowym\n",
    "#zawiera informację o bezpośredniej i pośredniej zależności\n",
    "plot_acf(series,lags=3649)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja ACF dla pierwszych 50 obserwacji (lagów)\n",
    "plot_acf(series, lags=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eea33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja PACF może być liczona maksymalnie dla połowy obserwacji, ale lepiej wybrać dużo mniej obserwacji\n",
    "print(3650/2-1)\n",
    "\n",
    "plot_pacf(series, lags=1250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc76245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wiemy, że PACF opisuje tylko bezpośredni związek między obserwacją a jej opóźnieniem\n",
    "#zatem nie obserwuje się korelacji dla wartości opóźnień powyżej jakiegoś k\n",
    "plot_pacf(series, lags=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697161f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja określająca jakość modelu ARIMA za pomocą metryki RMSE dla zadanych parametrów (p,d,q)\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    # przygotowanie zbioru treningowego i testowego\n",
    "    train_size = int(len(X) * 0.66)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    \n",
    "    # predykcja\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    \n",
    "    # wyznaczenie błędu predykcji\n",
    "    rmse = metr.mean_squared_error(test, predictions, squared=False)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ewaluacja modeli ARIMA z różnymi kombinacjami parametrów p, d, q \n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(dataset, order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, order\n",
    "                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA %s RMSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b24c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie danych\n",
    "def parser(x):\n",
    "    return datetime.strptime('202'+x, '%Y-%m')\n",
    "\n",
    "series = read_csv('dane/shampoo-sales.csv', header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n",
    "\n",
    "\n",
    "series.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81022387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF i PACF\n",
    "print(series.shape)\n",
    "\n",
    "plot_acf(series,lags=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ae63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(series,lags=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ewaluacja modeli (UWAGA - TRWA BARDZO DŁUGO - DO TESTÓW ZMNIEJSZAMY SIATKĘ)\n",
    "p_values = [0,1,2,3,4,5,6,7,8]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(series.values, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725ea1f",
   "metadata": {},
   "source": [
    "### https://facebook.github.io/prophet/\n",
    "### https://unit8co.github.io/darts/#\n",
    "### https://pycaret.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d233a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
